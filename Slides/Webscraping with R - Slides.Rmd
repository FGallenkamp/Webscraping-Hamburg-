---
title: "Web Scraping mit R"
author: "Fabian Gülzau (HU Berlin)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  slidy_presentation: default
subtitle: Workshop an der Universität Hamburg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
if(!require("pacman")) install.packages("pacman")
p_load(tidyverse, rvest, knitr, httr, kableExtra, robotstxt)
```

## Inhalt

 > "If programming is magic, then web scraping is wizardry" (Mitchell 2015: vii)

1. Einleitung
2. Technologien des WWW
3. Web Scraping (Toolbox)
4. Rechtliche Aspekte
5. Literatur(hinweise)
6. "hands-on"-Übungen

## 1. Einleitung

> "...something big is going on" (Salganik 2018: 2)

- Übergang vom analogen zum digitalen Zeitalter
    - Kommende Krise der empirischen Soziologie (Savage & Burrows 2007)?
    - neue Möglichkeiten und/oder Gefahren (Salganik 2018: 17-41)
    
> "'computational social science' (CSS) is occurring. The question is whether
> it happens with or without social scientists" (Heiberger & Riebling 2016: 1)
    
## Neue Möglichkeiten: Beispiele

Exemplarische Studien:

- King et al. (2013) ["How Censorship in China Allows Government Criticism but 
Silences Collective Expression"](http://bit.ly/2E1Yjhc)
- Budak & Watts (2015) ["Dissecting the Spirit of Gezi: Influence vs. Selection
in the Occupy Gezi Movement"](https://www.sociologicalscience.com/articles-v2-18-370/)
- Törnberg & Törnberg (2016) ["Combining CDA and topic modeling: Analyzing
discursive connections between Islamophobia and anti-feminism on an online forum"](https://journals.sagepub.com/doi/abs/10.1177/0957926516634546#articleShareContainer)

## Censorship in China

<div style="float: left; width: 50%;">
King et al. (2013):

- **Frage**: Was wird zensiert?
- **Vorgehen**: Online-Beiträge erheben und prüfen, welche zensiert werden
- **Daten**: 11 Millionen Beiträge auf 1,382 Internetseiten 
- **Ergebnisse**: Kritische Beiträge sind erlaubt; soziale Mobilisierung nicht
</div>

<div style="float: right; width: 50%;">
![<font size="3">Chacha: Chinesische Internetpolizei</font>](Figures/Chacha - Internet Police.png)
</div>

## Digitale Daten

- positiv: big, always-on, nonreactive
- negativ: incomplete, inaccessible, nonrepresentative, drifting, algorithmically 
confounded, sensitive

(Salganik 2018: 17-41)

## 2. Technologien des WWW

<div style="float: left; width: 70%;">
Infrastruktur des Internets im Alltag irrelevant.

Unser Browser übernimmt:

- Serveranfragen (Request/Response: HTTP)
- Darstellung von HTML, CSS und JavaScript

Um Informationen gezielt abzufragen, benötigen wir allerdings basale Kenntnisse 
der zugrundeliegenden Technologien.
</div>

<div style="float: right; width: 30%;">
![<font size="3">[Transatlantisches Seekabel](https://davegreer.cc/INTERNET)</font>](Figures/Hibernia Atlantic transoceanic cable landing.jpg)
</div>

## HTTP I

**H**yper**t**ext **T**ransfer **P**rotocol [(HTTP)](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol)

```{r}
# Source: https://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177
include_graphics("Figures/http-request-response.png")
```

- Übertragungsprotokoll, welches aus zwei Teilen besteht:

    - Request (Client)
    - Response (Server)

## HTTP II

- Requests erfolgen über **U**niform **R**esource **L**ocators [(URL)](https://en.wikipedia.org/wiki/URL)
    - Teile der URL können genutzt werden, um Informationen zu extrahieren
    
```{r}
# Source: https://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177
include_graphics("Figures/http-url-structure.png")
```

## Beispiel: HTTP I

- `GET`-Abfrage mit `httr::GET` und Antwort des Servers in R

```{r, echo = TRUE, eval = TRUE}
#install.packages("httr")
library(httr)
response <- GET("https://www.wiso.uni-hamburg.de/fachbereich-sozoek/professuren/lohmann/team/hertel-florian.html") %>%
  print()
```

## Beispiel: HTTP II

<div style="float: left; width: 70%;">
```{r, echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE}
#install.packages("rvest")
library(rvest)

pubs <- response %>% 
  read_html() %>% 
  html_nodes("#4128883 li") %>% 
  html_text() %>% 
  tibble(pub = .) %>%
  mutate(year = as.numeric(
    str_extract(pub, pattern = "[:digit:]{4}"))) %>%
  group_by(year) %>%
  summarise(number = n())
```
</div>

<div style="float: left; width: 30%;">
```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}
pubs %>%
  kable() %>%
  kable_styling()
```
</div>

## HTML

**H**yper**t**ext **M**arkup **L**anguage [(HTML)](https://www.w3schools.com/html/default.asp)

- einfacher Text mit weiteren Anweisungen (Tags, Attributes...)
    - wird vom Browser interpretiert und dargestellt
    
- HTML-Code ist über die Webentwicklungstools des Browsers verfügbar
    - Rechtsklick -> Element untersuchen (Firefox)

## Beispiel: HTML I

Einfache Seiten über Editor/Notepad++ erstellbar:

```{r, eval = FALSE, echo = TRUE}
<!DOCTYPE html>
<html>

<head>
<title>Workshop Web Scraping</title>
</head>

<body>
<h1> Web Scraping mit R, Universit&auml;t Hamburg</h1>
<p>Dieser Kurs f&uuml;hrt in das "Web Scraping" mit R ein. Er wird durch <a href="https://fguelzau.rbind.io/">Fabian G&uuml;lzau</a> geleitet.</p>

</body>
</html>
```

Der Browser interpretiert den Code und zeigt eine [Internetseite](http://htmlpreview.github.io/?https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/HTML-Example.html) an.

## Beispiel: HTML II

HTML:

- hat eine Baumstruktur
- Markup-Elemente helfen uns Informationen zu lokalisieren (Relationalität)
    - **D**ocument **O**bject **M**odel [(DOM)](https://en.wikipedia.org/wiki/Document_Object_Model)
    - Darstellung von HTML in R (parsing)
    
```{r}
# Source: https://www.wiso.uni-hamburg.de/fachbereich-sozoek/professuren/lohmann/team/hertel-florian.html
include_graphics("Figures/TreeStructure.png")
```

## CSS

**C**ascading **S**tyle **S**heet [(CSS)](https://www.w3schools.com/css/default.asp)

- beinhaltet Informationen über den Stil der HTML-Seite
- kann genutzt werden, um Informationen zu lokalisieren (CSS-Selektoren)

```{r}
# Source: https://en.wikipedia.org/wiki/Cascading_Style_Sheets
include_graphics("Figures/CSS-Example.png")
```

## JavaScript

[JavaScript](https://www.w3schools.com/js/default.asp): Programmiersprache des Internets

- macht Internetseiten dynamisch
- Inhalte erscheinen erst nach Ausführung von JS 
    - problematisch für unsere Scraper

```{r}
# Source: https://pudding.cool/2018/12/3d-cities-story/
include_graphics("Figures/JavaScript-Example.png")
```

Quelle: [The Pudding (2018)](https://pudding.cool/2018/12/3d-cities-story/)

## 3. Web Scraping (Toolbox)

Web Scraping-Projekte folgen einem generellen Schema:

1. Internetseite kennenlernen
2. Import von HTML-Dateien
3. Information isolieren
4. Iteration (loops)

```{r}
# Source: https://r4ds.had.co.nz/introduction.html
include_graphics("Figures/data-science-workflow.png")
```

Quelle: [Wickham & Grolemund (2018)](https://r4ds.had.co.nz/introduction.html)

## Web Scraping (Toolbox)

<div style="float: left; width: 70%;">
Das generelle Schema lässt sich in R über das Paket [`rvest`](https://github.com/hadley/rvest) umsetzen:

1. Kennenlernen der Internetseite [(u.a. `robotstxt`)](https://github.com/ropensci/robotstxt)
2. `read_html`: Import von HTML/XML-Dateien
3. Information isolieren
    - `html_nodes`: Extrahiert Teile des HTML-Dokument mit Hilfe von XPath/CSS-Selektoren
    - `html_text` / `html_attr` / `html_table`: Extrahiert Text, Attribute und Tabellen
4. `map` (package: [purrr](https://purrr.tidyverse.org/)): Iteration (loops)
</div>

<div style="float: right; width: 30%;">
```{r}
# Source: http://hexb.in/
include_graphics("Figures/rvest.png")
```
</div>

## Beispiel: Sonntagsfrage

- Sonntagsfrage
    - von verschiedenen Umfrageinstituten erhoben [(Wahlrecht.de)](https://www.wahlrecht.de/umfragen/)
    - Archivfunktion (zeitlicher Verlauf)
    
Ziel: Aktuelle Umfragen aller Institute herunterladen und kombinieren.

```{r}
# Source: https://www.tagesschau.de/inland/deutschlandtrend/
include_graphics("Figures/Sonntagsfrage.png")
```

Quelle: [Tagesschau.de](https://www.tagesschau.de/inland/deutschlandtrend/)

## (1) Kennenlernen der Internetseite

Fragen, die beantwortet werden sollten:

- statische oder dynamische Internetseite
    - statisch: [`rvest`](https://github.com/hadley/rvest)
    - dynamisch: [`RSelenium`](https://github.com/ropensci/RSelenium)
- Web Scraping erlaubt
    - Terms of Service (ToS)
    - robots.txt

## Wahlrecht: Kennenlernen der Internetseite

Hierzu gehört:

- Seitenquelltext: **statisch**
- HTML-Tags der Zielinformation: **table / class: wilko**
- robots.txt / Terms of Use: **keine relevanten Beschränkungen**

```{r echo = TRUE, eval = TRUE}
paths_allowed(
  paths  = c("/index.htm","/allensbach.htm"), 
  domain = c("wahlrecht.de")
)
```

## (2) Import von HTML-Dateien

Internetseiten müssen in ein Format übersetzt werden, welches von R gelesen und
bearbeitet werden kann (z.B. Baumstruktur).

Benötigt wird:

- URL 
- -> Request/Response-Paar

## Wahlrecht: Import von HTML-Seiten

Über `rvest::read_html`:

```{r echo = TRUE, eval = TRUE}
(html.page <- read_html("https://www.wahlrecht.de/umfragen/allensbach.htm"))
```

## (3) Information isolieren

Zur Extraktion von Informationen nutzen wir die Baumstruktur von HTML:

- [XPath](https://www.w3schools.com/xml/xpath_intro.asp)
- [CSS-Selektoren](https://www.w3schools.com/csSref/css_selectors.asp)

```{r, out.width = "90%", out.height = "95%"}
# Source: https://wiki.selfhtml.org/wiki/XML/Regeln/Baumstruktur
include_graphics("Figures/TreeStructureII.png")
```

Quelle: [selfhtml.de](https://wiki.selfhtml.org/wiki/XML/Regeln/Baumstruktur)

## XPath und CSS-Selektoren: Tools

Wir konstruieren Selektoren selten manuell, da Anwendungen dies übernehmen.

Tools:

- [Selector Gadget](https://selectorgadget.com/)
- Browser: [Webentwicklungstools](https://developer.mozilla.org/de/docs/Tools/Seiten_Inspektor)
- Lerntools: [CSS Diner](https://flukeout.github.io/)

## Wahlrecht: CSS-Selektor

HTML-Tabellen lassen sich oft besonders leicht identifizieren, da sie das Tag 
"table" tragen: 

```{r echo = TRUE, eval = TRUE}
(html.node <- html_nodes(html.page, css = ".wilko")) 
```

## Wahlrecht: Umwandeln in Text/Tabelle

Wir sind selten am HTML-Tag interessiert, sondern an dem Inhalt:

```{r echo = TRUE, eval = TRUE}
html.table <- html.node %>%
  html_table(header = TRUE, fill = TRUE) %>%
  .[[1]] %>%                                 # body
  .[4:nrow(.), c(1, 3:9)] %>%                # subsetting
  glimpse()
```

## Exkurs: Regex

<div style="float: left; width: 70%;">
**Reg**ular **Ex**pression

- zur Suche von Ausdrücken (patterns) in Text (strings)
- in R z.B. über [`stringr`](https://stringr.tidyverse.org/index.html)

```{r echo = TRUE, eval = TRUE}
str_view(string = "Wir benötigen nicht den gesamten Text, 
         sondern nur die Zahl, welche sich im Text verbirgt: 42.",
         pattern = "[:digit:]+")
```
</div>

<div style="float: left; width: 30%;">
```{r, out.width = "90%", out.height = "95%"}
# Source: https://boyter.org/2016/04/collection-orly-book-covers/
include_graphics("Figures/RegexOrly.jpg")
```
</div>

## Wahlrecht: Regex

Die Umfrageergebnisse liegen als "strings" vor, sodass wir sie für die Datenanalyse 
in numerische Werte umwandeln müssen.

```{r echo = TRUE, eval = TRUE}
str_view(string = html.table$`CDU/CSU`[1:5],
         pattern = "[:digit:]+,?[:digit:]?")
```

## Datenaufbereitung

Wir ersetzen zudem die Kommata durch Punkte und wandeln die Daten in ein 
"long"-Format um:

```{r echo = TRUE, eval = TRUE}
allensbach.df <- html.table %>%
  rename("Zeitpunkt" = 1) %>%                                 # 1. Variable benennen                    
  mutate(Zeitpunkt = parse_datetime(Zeitpunkt,                # 2. als Datum
                                    format = "%d.%m.%Y")) %>%
  mutate_if(is.character, str_extract,                        # 3a. Zahl entnehmen
            pattern = "[:digit:]+,?[:digit:]?") %>%
  mutate_if(is.character, str_replace,                        # 3b. Komma als Punkt
            pattern = ",", replacement = ".") %>%
  mutate_if(is.character, as.numeric) %>%                     # 3c. als Zahl
  gather(party, vote, -Zeitpunkt) %>%                         # 4. long format
  glimpse()                                                   # 5. ausgeben
```

## Visualisierung

Zuletzt können wir die Ergebnisse der Sonntagsfrage visualisieren (Paket: [`ggplot2`](https://ggplot2.tidyverse.org/)):

```{r echo = TRUE, eval = TRUE}
ggplot(subset(allensbach.df, party != "Sonstige"),
       aes(x = Zeitpunkt, y = vote, colour = party)) +
  geom_line(size = 1.5) +
  scale_color_manual(values = c("#009EE0", "#000000", "#FFED00", "#64A12D",
                                "#BE3075", "#EB001F"),
                     guide = guide_legend(title = NULL)) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  labs(title = "Sonntagsfrage: Wenn am nächsten Sonntag Bundestagswahl wäre...",
       x = "", y = "", caption = "Quelle: wahlrecht.de (Allensbach)") +
  theme_minimal()
```

## 4. Rechtliche Aspekte

> "Big data? Cheap. Lawyers? Not so much." (Pete Warden zit. in Mitchell 2015: 217)

- rechtliche Grauzone
- länderspezifisch (USA: strikt; Deutschland: liberal)
- Terms of Service (ToS) beachten
- robots.txt prüfen

## Rechtliche Aspekte in der Praxis

Ziel: "friendly scraping" 

- Server nicht überlasten ([crawl delay](https://rud.is/b/2017/07/28/analyzing-wait-delay-settings-in-common-crawl-robots-txt-data-with-r/))
    - `Sys.sleep` einbauen (~5-10 Sekunden)
    - zufällige Wartezeit, um menschlisches Verhalten zu imitieren
- Bot oder Mensch
    - headless browsing
        - [RSelenium](https://github.com/ropensci/RSelenium), [decapitated](https://github.com/hrbrmstr/decapitated), [splashr](https://github.com/hrbrmstr/splashr) oder [htmlunit](https://github.com/hrbrmstr/htmlunit)
- API vorhanden (Sammlung unter [programmableweb.com](https://www.programmableweb.com/))
- Seitenbetreiber kontaktieren

## 5. Literatur(hinweise)

