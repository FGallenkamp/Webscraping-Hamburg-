---
title: "Web Scraping mit R"
author: "Fabian Gülzau (HU Berlin)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  slidy_presentation: default
subtitle: Workshop an der Universität Hamburg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
if(!require("pacman")) install.packages("pacman")
p_load(tidyverse, rvest, knitr, httr, kableExtra, robotstxt, eurostat, lubridate)
```

## Inhalt

 > "If programming is magic, then web scraping is wizardry" ([Mitchell 2015: vii](http://shop.oreilly.com/product/0636920034391.do))

1. Einleitung
2. Technologien des WWW
3. Web Scraping (Toolbox)
    a. Generelles Schema
    b. Spezielle Technologien
4. Rechtliche Aspekte
5. Weitere Beispiele und Anwendungen

## Zur Präsentation

- Folien sind online verfügbar: https://goo.gl/erTCX7
    - Folien im Browser öffnen
- Benötigte Pakete über: 

```{r echo = TRUE, eval = FALSE}
install.packages(pacman) # Installation nur einmal notwendig
library(pacman)
p_load(tidyverse, rvest, httr, robotstxt, eurostat, lubridate)
```

## 1. Einleitung

> "...something big is going on" ([Salganik 2018: 2](https://www.bitbybitbook.com/en/1st-ed/preface/))

- Übergang vom analogen zum digitalen Zeitalter
    - kommende Krise der empirischen Soziologie ([Savage & Burrows 2007](https://journals.sagepub.com/doi/10.1177/0038038507080443#articleShareContainer))?
    - neue Möglichkeiten und/oder Gefahren ([Salganik 2018: 17-41](https://www.bitbybitbook.com/en/1st-ed/observing-behavior/characteristics/))
    
> "'computational social science' (CSS) is occurring. The question is whether
> it happens with or without social scientists" ([Heiberger & Riebling 2016: 1](https://journals.sagepub.com/doi/abs/10.1177/2059799115622763))
    
## Neue Möglichkeiten: Beispiele

Exemplarische Studien:

- King et al. (2013) ["How Censorship in China Allows Government Criticism but 
Silences Collective Expression"](http://bit.ly/2E1Yjhc)
- Budak & Watts (2015) ["Dissecting the Spirit of Gezi: Influence vs. Selection
in the Occupy Gezi Movement"](https://www.sociologicalscience.com/articles-v2-18-370/)
- Törnberg & Törnberg (2016) ["Combining CDA and topic modeling: Analyzing
discursive connections between Islamophobia and anti-feminism on an online forum"](https://journals.sagepub.com/doi/abs/10.1177/0957926516634546#articleShareContainer)

## Zensur in China I

<div style="float: left; width: 50%;">
King et al. ([2013](http://bit.ly/2E1Yjhc)):

- **Frage**: Was wird zensiert?
- **Vorgehen**: Online-Beiträge erheben und prüfen, welche zensiert werden
- **Daten**: 11 Millionen Beiträge auf 1,382 Internetseiten 
- **Ergebnisse**: Kritische Beiträge sind erlaubt; soziale Mobilisierung nicht
</div>

<div style="float: right; width: 50%;">
![<font size="3">Chacha: Chinesische Internetpolizei</font>](Figures/Chacha - Internet Police.png)
</div>

## Zensur in China II

```{r, out.width = "100%"}
# Source: King et al. (2013: 335)
include_graphics("Figures/King et al 2013 - p335.png")
```

Quelle: [King et al. (2013: 335)](http://bit.ly/2E1Yjhc)

## Lehre

Studium.org ([Code](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/SoziologieOrg-Scraper.R)):

- Informationen zu Soziologiestudiengängen
- z.B. Mietspiegel, Wetter, Kinos...

```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
uni.df <- readRDS(file = "C:/Users/guelzauf/Seafile/Meine Bibliothek/Projekte/Webscraping-Hamburg-/Slides/Figures/StudiumOrg.RDS")

ggplot(uni.df) +
  geom_point(aes(fct_reorder(name, sonnenstunden_pro_jahr), sonnenstunden_pro_jahr), 
             stat = "identity") +
  labs(title = "Deutsche Soziologieinstitute nach Sonnenstunden im Jahr",
       subtitle = paste0("Die Differenz zwischen Berlin und Hamburg beträgt ", 
                         round(
                           abs(
                             uni.df$sonnenstunden_pro_jahr[uni.df$name == "uni-hamburg"] -
                               uni.df$sonnenstunden_pro_jahr[uni.df$name == "hu-berlin"]) /
                             24, 
                           digits = 1),
                         " Sonnentage im Jahr."),
       x = "", y = "", caption = "Quelle: studium.org") +
  geom_point(data = subset(uni.df, name %in% c("uni-hamburg", "hu-berlin")),
             aes(fct_reorder(name, sonnenstunden_pro_jahr), sonnenstunden_pro_jahr), 
             stat = "identity", color = "red", size = 4) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.ticks = element_line())
```

## Digitale Daten

- positiv: big, always-on, nonreactive
- negativ: incomplete, inaccessible, nonrepresentative, drifting, algorithmically 
confounded, sensitive

Quelle: [Salganik 2018: 17-41](https://www.bitbybitbook.com/en/1st-ed/observing-behavior/characteristics/)

## 2. Technologien des WWW

<div style="float: left; width: 70%;">
Infrastruktur des Internets im Alltag irrelevant.

Unser Browser übernimmt:

- Serveranfragen (Request/Response: HTTP)
- Darstellung von HTML, CSS und JavaScript

Um Informationen gezielt abzufragen, benötigen wir allerdings basale Kenntnisse 
der zugrundeliegenden Technologien.
</div>

<div style="float: right; width: 30%;">
![<font size="3">[Transatlantisches Seekabel](https://davegreer.cc/INTERNET)</font>](Figures/Hibernia Atlantic transoceanic cable landing.jpg)
</div>

## HTTP I

**H**yper**t**ext **T**ransfer **P**rotocol [(HTTP)](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol)

```{r, out.width = "50%"}
# Source: https://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177
include_graphics("Figures/http-request-response.png")
```

- Übertragungsprotokoll, welches aus zwei Teilen besteht:

    - Request (Client)
    - Response (Server)

## HTTP II

- Requests erfolgen über **U**niform **R**esource **L**ocators [(URL)](https://en.wikipedia.org/wiki/URL)
    - Teile der URL können genutzt werden, um Informationen zu extrahieren
    
```{r, out.width = "50%"}
# Source: https://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177
include_graphics("Figures/http-url-structure.png")
```

## Beispiel: HTTP I

- `GET`-Abfrage mit `httr::GET` und Antwort des Servers in R

```{r, echo = TRUE, eval = TRUE}
#install.packages("httr")
library(httr)
response <- GET("https://www.wiso.uni-hamburg.de/fachbereich-sozoek/professuren/lohmann/team/hertel-florian.html") %>%
  print()
```

## Beispiel: HTTP II

<div style="float: left; width: 70%;">
```{r, echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE}
#install.packages("rvest")
library(rvest)

pubs <- response %>% 
  read_html() %>% 
  html_nodes("#4128883 li") %>% 
  html_text() %>% 
  tibble(pub = .) %>%
  mutate(year = as.numeric(
    str_extract(pub, pattern = "[:digit:]{4}"))) %>%
  group_by(year) %>%
  summarise(number = n())
```
</div>

<div style="float: left; width: 30%;">
```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}
pubs %>%
  kable() %>%
  kable_styling()
```
</div>

## HTML

**H**yper**t**ext **M**arkup **L**anguage [(HTML)](https://www.w3schools.com/html/default.asp)

- einfacher Text mit weiteren Anweisungen (Tags, Attributes...)
    - wird vom Browser interpretiert und dargestellt
    
- HTML-Code ist über die Webentwicklungstools des Browsers verfügbar
    - Rechtsklick -> Element untersuchen (Firefox)

## Beispiel: HTML I

Einfache Seiten über Editor/Notepad++ erstellbar:

```{r, eval = FALSE, echo = TRUE}
<!DOCTYPE html>
<html>

<head>
<title>Workshop Web Scraping</title>
</head>

<body>
<h1> Web Scraping mit R, Universit&auml;t Hamburg</h1>
<p>Dieser Kurs f&uuml;hrt in das "Web Scraping" mit R ein. Er wird durch <a href="https://fguelzau.rbind.io/">Fabian G&uuml;lzau</a> geleitet.</p>

</body>
</html>
```

Der Browser interpretiert den Code und zeigt eine [Internetseite](http://htmlpreview.github.io/?https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/HTML-Example.html) an.

## Beispiel: HTML II

HTML:

- hat eine Baumstruktur
- Markup-Elemente helfen uns Informationen zu lokalisieren (Relationalität)
    - **D**ocument **O**bject **M**odel [(DOM)](https://en.wikipedia.org/wiki/Document_Object_Model)
    - Darstellung von HTML in R (parsing)
    
```{r, out.width = "70%"}
# Source: https://www.wiso.uni-hamburg.de/fachbereich-sozoek/professuren/lohmann/team/hertel-florian.html
include_graphics("Figures/TreeStructure.png")
```

## CSS

**C**ascading **S**tyle **S**heet [(CSS)](https://www.w3schools.com/css/default.asp)

- beinhaltet Informationen über den Stil der HTML-Seite
- kann genutzt werden, um Informationen zu lokalisieren (CSS-Selektoren)

```{r, out.width = "30%"}
# Source: https://en.wikipedia.org/wiki/Cascading_Style_Sheets
include_graphics("Figures/CSS-Example.png")
```

## JavaScript

[JavaScript](https://www.w3schools.com/js/default.asp): Programmiersprache des Internets

- macht Internetseiten dynamisch
- Inhalte erscheinen erst nach Ausführung von JS 
    - problematisch für unsere Scraper

```{r, out.width = "50%"}
# Source: https://pudding.cool/2018/12/3d-cities-story/
include_graphics("Figures/JavaScript-Example.png")
```

Quelle: [The Pudding (2018)](https://pudding.cool/2018/12/3d-cities-story/)

## 3a. Web Scraping (Toolbox)

Web Scraping-Projekte folgen einem generellen Schema:

1. Internetseite kennenlernen
2. Import von HTML-Dateien
3. Information isolieren
4. Iteration (loops)

```{r, out.width = "50%"}
# Source: https://r4ds.had.co.nz/introduction.html
include_graphics("Figures/data-science-workflow.png")
```

Quelle: [Wickham & Grolemund (2018)](https://r4ds.had.co.nz/introduction.html)

## Web Scraping (Toolbox)

<div style="float: left; width: 70%;">
Das generelle Schema lässt sich in R über das Paket [`rvest`](https://github.com/hadley/rvest) umsetzen:

1. Kennenlernen der Internetseite [(u.a. `robotstxt`)](https://github.com/ropensci/robotstxt)
2. `read_html`: Import von HTML/XML-Dateien
3. Information isolieren
    - `html_nodes`: Extrahiert Teile des HTML-Dokument mit Hilfe von XPath/CSS-Selektoren
    - `html_text` / `html_attr` / `html_table`: Extrahiert Text, Attribute und Tabellen
4. `map` (package: [purrr](https://purrr.tidyverse.org/)): Iteration (loops)
</div>

<div style="float: right; width: 30%;">
```{r, out.width = "50%"}
# Source: http://hexb.in/
include_graphics("Figures/rvest.png")
```
</div>

## Beispiel: Sonntagsfrage

- Sonntagsfrage
    - von verschiedenen Umfrageinstituten erhoben [(Wahlrecht.de)](https://www.wahlrecht.de/umfragen/)
    - Archivfunktion (zeitlicher Verlauf)
    
Ziel: Aktuelle Umfragen aller Institute herunterladen und kombinieren.

```{r, out.width = "50%"}
# Source: https://www.tagesschau.de/inland/deutschlandtrend/
include_graphics("Figures/Sonntagsfrage.png")
```

Quelle: [Tagesschau.de](https://www.tagesschau.de/inland/deutschlandtrend/)

## (1) Kennenlernen der Internetseite

Fragen, die beantwortet werden sollten:

- statische oder dynamische Internetseite
    - statisch: [`rvest`](https://github.com/hadley/rvest)
    - dynamisch: [`RSelenium`](https://github.com/ropensci/RSelenium)
- Web Scraping erlaubt
    - Terms of Service (ToS)
    - robots.txt

## Wahlrecht: Kennenlernen der Internetseite

Hierzu gehört:

- Seitenquelltext: **statisch**
- HTML-Tags der Zielinformation: **table / class: wilko**
- robots.txt / Terms of Service: **keine relevanten Beschränkungen**

```{r echo = TRUE, eval = TRUE}
paths_allowed(
  paths  = c("/index.htm","/allensbach.htm"), 
  domain = c("wahlrecht.de")
)
```

## (2) Import von HTML-Dateien

Internetseiten müssen in ein Format übersetzt werden, welches von R gelesen und
bearbeitet werden kann (z.B. Baumstruktur).

Benötigt wird:

- URL 
- -> Request/Response-Paar

## Wahlrecht: Import von HTML-Seiten

Über `rvest::read_html`:

```{r echo = TRUE, eval = TRUE}
(html.page <- read_html("https://www.wahlrecht.de/umfragen/allensbach.htm"))
```

## (3) Information isolieren

Zur Extraktion von Informationen nutzen wir die Baumstruktur von HTML:

- [XPath](https://www.w3schools.com/xml/xpath_intro.asp)
- [CSS-Selektoren](https://www.w3schools.com/csSref/css_selectors.asp)

```{r, out.width = "80%", out.height = "85%"}
# Source: https://wiki.selfhtml.org/wiki/XML/Regeln/Baumstruktur
include_graphics("Figures/TreeStructureII.png")
```

Quelle: [selfhtml.de](https://wiki.selfhtml.org/wiki/XML/Regeln/Baumstruktur)

## XPath und CSS-Selektoren: Tools

Wir konstruieren Selektoren selten manuell, da Anwendungen dies übernehmen.

Tools:

- [Selector Gadget](https://selectorgadget.com/)
- Browser: [Webentwicklungstools](https://developer.mozilla.org/de/docs/Tools/Seiten_Inspektor)
- Lerntools: [CSS Diner](https://flukeout.github.io/)

## Wahlrecht: CSS-Selektor

HTML-Tabellen lassen sich oft besonders leicht identifizieren, da sie das Tag 
"table" tragen: 

```{r echo = TRUE, eval = TRUE}
(html.node <- html_nodes(html.page, css = ".wilko")) 
```

## Wahlrecht: Umwandeln in Text/Tabelle

Wir sind selten am HTML-Tag interessiert, sondern an dem Inhalt:

```{r echo = TRUE, eval = TRUE}
html.table <- html.node %>%
  html_table(header = TRUE, fill = TRUE) %>%
  .[[1]] %>%                                 # body
  .[4:nrow(.), c(1, 3:9)] %>%                # subsetting
  glimpse()
```

## Exkurs: Regex

<div style="float: left; width: 70%;">
**Reg**ular **Ex**pression

- zur Suche von Ausdrücken (patterns) in Text (strings)
- in R z.B. über [`stringr`](https://stringr.tidyverse.org/index.html)

```{r echo = TRUE, eval = TRUE}
str_view(string = "Wir benötigen nicht den gesamten Text, 
         sondern nur die Zahl, welche sich im Text verbirgt: 42.",
         pattern = "[:digit:]+")
```
</div>

<div style="float: left; width: 30%;">
```{r, out.width = "90%", out.height = "95%"}
# Source: https://boyter.org/2016/04/collection-orly-book-covers/
include_graphics("Figures/RegexOrly.jpg")
```
</div>

## Wahlrecht: Regex

Die Umfrageergebnisse liegen als "strings" vor, sodass wir sie für die Datenanalyse 
in numerische Werte umwandeln müssen.

```{r echo = TRUE, eval = TRUE}
str_view(string = html.table$`CDU/CSU`[1:5],
         pattern = "[:digit:]+,?[:digit:]?")
```

## Datenaufbereitung

Wir ersetzen zudem die Kommata durch Punkte und wandeln die Daten in ein 
"long"-Format um:

```{r echo = TRUE, eval = TRUE}
allensbach.df <- html.table %>%
  rename("Zeitpunkt" = 1) %>%                                 # 1. Variable benennen                    
  mutate(Zeitpunkt = parse_datetime(Zeitpunkt,                # 2. als Datum
                                    format = "%d.%m.%Y")) %>%
  mutate_if(is.character, str_extract,                        # 3a. Zahl entnehmen
            pattern = "[:digit:]+,?[:digit:]?") %>%
  mutate_if(is.character, str_replace,                        # 3b. Komma als Punkt
            pattern = ",", replacement = ".") %>%
  mutate_if(is.character, as.numeric) %>%                     # 3c. als Zahl
  gather(party, vote, -Zeitpunkt) %>%                         # 4. long format
  glimpse()                                                   # 5. ausgeben
```

## Visualisierung

Zuletzt können wir die Ergebnisse der Sonntagsfrage visualisieren (Paket: [`ggplot2`](https://ggplot2.tidyverse.org/)):

```{r echo = TRUE, eval = TRUE}
ggplot(subset(allensbach.df, party != "Sonstige"),
       aes(x = Zeitpunkt, y = vote, colour = party)) +
  geom_line(size = 1.5) +
  scale_color_manual(values = c("#009EE0", "#000000", "#FFED00", "#64A12D",
                                "#BE3075", "#EB001F"),
                     guide = guide_legend(title = NULL)) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  labs(title = "Sonntagsfrage: Wenn am nächsten Sonntag Bundestagswahl wäre...",
       x = "", y = "", caption = "Quelle: wahlrecht.de (Allensbach)") +
  theme_minimal()
```

## (4) Iteration

Wird in den weiteren Anwendungen besprochen ([Code](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/WahlrechtDe-Scraper.R))

```{r echo = FALSE, eval = TRUE}
institute.href <- read_html("https://www.wahlrecht.de/umfragen/index.htm") %>%
  html_nodes(".in a") %>%
  html_attr("href") %>%
  tibble(institute = str_extract(., "[:alpha:]+(?=.)"),
         link = paste0("https://www.wahlrecht.de/umfragen/", .)) %>%
  rename("href" = 1)

# For filtering
parties <- c("CDU/CSU", "SPD", "GRÜNE", "FDP", "LINKE", "AfD") 

# (2) Create a function that grabs the tables 
poll_scraper <- function(x) {
  read_html(x) %>%
    html_nodes(".wilko") %>%
    html_table(fill = TRUE, header = TRUE) %>%
    .[[1]] %>%
    .[min(                                                                         # Which row is the first one with a date? 
      which(
        str_detect(
          string = .[,1], "[:digit:]+.?[:digit:]+.?[:digit:]+") == TRUE)):nrow(.), 
      c(1, which(colnames(.) %in% parties))] %>%                                   
    rename("Zeitpunkt" = 1) %>%
    mutate(Zeitpunkt = parse_datetime(Zeitpunkt, format = "%d.%m.%Y")) %>%
    mutate_if(is.character, str_extract, pattern = "[:digit:]+,?[:digit:]?") %>%
    mutate_if(is.character, str_replace, pattern = ",", replacement = ".") %>%
    mutate_if(is.character, as.numeric) %>%
    gather(party, vote, -Zeitpunkt)
}

# (3) Grab all polls of all institute
polls <- map(institute.href$link, poll_scraper) %>% 
  setNames(institute.href$institute) # Name the list

# (4) Name lists and combine in a data set
polls.df <- polls %>%
  bind_rows(.id = "institute")

# Visualize
ggplot(subset(polls.df, year(Zeitpunkt) > 2014),
       aes(x = Zeitpunkt, y = vote, colour = party)) +
  geom_line(size = 1.5) +
  facet_wrap(~institute) +
  scale_color_manual(values = c("#009EE0", "#000000", "#FFED00", "#64A12D",
                                "#BE3075", "#EB001F"),
                     guide = guide_legend(title = NULL)) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  labs(title = "Sonntagsfrage: Wenn am nächsten Sonntag Bundestagswahl wäre...",
       x = "", y = "", caption = "Quelle: wahlrecht.de") +
  theme_minimal()
```

## 3b. Spezielle Technologien

Spezielle Möglichkeiten & Herausforderungen:

[**A**pplication **P**rogramming **I**nterface (API)](https://en.wikipedia.org/wiki/Application_programming_interface)

- **erleichtert** den Zugriff auf Daten über spezifische Schnittstellen
    - Beispiele: [Twitter](https://developer.twitter.com/en.html), [Die Zeit](http://developer.zeit.de/index/), [Eurostat](https://ec.europa.eu/eurostat/web/json-and-unicode-web-services), [Wikipedia](https://github.com/Ironholds/WikipediR/)
    
Dynamische Webseiten & bot blocking:

- **erschwert** Zugriff, da Daten erst nach Nutzeranfragen erzeugt werden
    - Beispiele: Süddeutsche Zeitung (Archiv), ArtFacts

## APIs: Steigende Relevanz

```{r echo = FALSE, eval = TRUE}
api.plot.df <- readRDS(file = "C:/Users/guelzauf/Seafile/Meine Bibliothek/Projekte/Webscraping-Hamburg-/Slides/Figures/api_plot_df.RDS")

ggplot(api.plot.df, aes(x = Submitted, y = cumulative.n)) +
  geom_line(stat = "identity") +
  theme_minimal() +
  labs(title = "Anzahl and APIs auf programmableweb.com",
       subtitle = "Eine steigende Anzahl an Unternehmen bietet APIs an.",
       caption = "Quelle: programmableweb.org",
       x = "", 
       y = "") +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        text = element_text(size = 14),
        axis.ticks = element_line(size = .5))
```

Quelle: Eigene Darstellung [(Code)](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/programmableweb-Scraper.R)

## API: Eurostat

Eurostat bietet eine API für Datenbankabfragen an (Paket: [`eurostat`](https://ropengov.github.io/eurostat/))

Es gibt ein ["Cheatsheet"](https://github.com/rstudio/cheatsheets/raw/master/eurostat.pdf) für das Paket, welches das folgende Vorgehen beschreibt:

1. Suche nach [Daten](https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?sort=1&file=table_of_contents_en.pdf) (`search_eurostat`) 
2. Herunterladen der Daten (`get_eurostat`)
3. Datenaufbereitung (`tidyverse`)

## Beispiel: Eurostat

Arbeitslosenquote:

1. Daten suchen

```{r echo = TRUE, eval = TRUE, warning = FALSE}
content <- search_eurostat(pattern = "unemployment rate", type = "dataset") %>%
  glimpse()
```

2. Daten herunterladen

```{r echo = TRUE, eval = TRUE, warning = FALSE}
unem.df <- get_eurostat(id = content$code[1],                  # Variable
                        time_format = "date",                  # Datumsformat
                        filters = list(s_adj = "NSA",          # Datenfilter
                                       indic = "LM-UN-T-TOT", 
                                       geo = "EU28"),
                        stringsAsFactors = FALSE) 
```

3. Datenaufbereitung

```{r echo = TRUE, eval = TRUE, warning = FALSE}
# 3. Datenaufbereitung
unem.df %>%
  filter(time >= "2018-01-01") %>%
  ggplot() +
  geom_line(mapping = aes(x = time, y = values), stat = "identity") +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  theme_minimal() +
  labs(title = "Monatliche Arbeitslosenquote (EU28), 2018",
       caption = "Quelle: eurostat (LM-UN-T-TOT)",
       x = "", 
       y = "")
```

## 4. Rechtliche Aspekte

> "Big data? Cheap. Lawyers? Not so much." (Pete Warden zit. in [Mitchell 2015: 217](http://shop.oreilly.com/product/0636920034391.do))

- rechtliche Grauzone
- länderspezifisch (USA: strikt; Deutschland: liberal)
- Terms of Service (ToS) beachten
- robots.txt prüfen

## 4. Rechtliche Aspekte in der Praxis

Ziel: "friendly scraping" 

- Server nicht überlasten ([crawl delay](https://rud.is/b/2017/07/28/analyzing-wait-delay-settings-in-common-crawl-robots-txt-data-with-r/))
    - `Sys.sleep` einbauen (~5-10 Sekunden)
    - zufällige Wartezeit, um menschliches Verhalten zu imitieren
- Bot oder Mensch
    - "headless browsing"
        - [RSelenium](https://github.com/ropensci/RSelenium), [decapitated](https://github.com/hrbrmstr/decapitated), [splashr](https://github.com/hrbrmstr/splashr) oder [htmlunit](https://github.com/hrbrmstr/htmlunit)
- API nutzen (Sammlung unter [programmableweb.com](https://www.programmableweb.com/))
- Seitenbetreiber kontaktieren

## 5. "hands-on"-Übungen

Weitere Beispiele:

- statische Seiten
    - [Gründerszene.de](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/GruenderszeneDe-Scraper.R)
    - [Wahlrecht.de](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/WahlrechtDe-Scraper.R)
    - [Studium.org](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/SoziologieOrg-Scraper.R) 
    
- dynamische Seiten
    - [ArtFacts.net](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/ArtFacts-Scraper.R)

- APIs
    - [Twitter](https://github.com/FabianFox/Webscraping-Hamburg-/blob/master/Code/TwitterExample.R)
    
## 5. R-Setup

- benötigen [R](https://www.r-project.org/) und [RStudio](https://www.rstudio.com/products/rstudio/download/)
- Alternative: [RStudio Cloud](https://rstudio.cloud/)
    - derzeit kostenlos
    - läuft über den Browser (es muss nichts installiert werden)
    - (Einige) Beispiele können ausgeführt werden